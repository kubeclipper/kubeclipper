# 创建集群-添加一个节点-移除一个节点-添加2个节点-批量移除节点-删除集群

fixtures:
  - ConfigFixture
  - SampleDataFixture

defaults:
  ssl: False
  request_headers:
    content-type: application/json
    accept: application/json

vars:
  - &username "admin"
  - &password "Thinkbig1"
  - &cluster_name "add_remove_nodes_cluster"

tests:
  - name: user_login
    url: /apis/oauth/login
    method: POST
    data:
      username: *username
      password: *password
    status: 200
    response_json_paths:
      $.token_type: Bearer

  - name: get_available_node
    url: /apis/api/core.kubeclipper.io/v1/nodes
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    query_parameters:
      labelSelector: topology.kubeclipper.io/region=default,!kubeclipper.io/nodeRole
      limit: -1
      page: 1
    status: 200

  - name: create_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters
    method: POST
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data:
      addons: []
      apiVersion: "core.kubeclipper.io/v1"
      certSANs: []
      cni:
        calico:
          IPManger: true
          IPv4AutoDetection: "first-found"
          IPv6AutoDetection: ""
          mode: "Overlay-Vxlan-All"
          mtu: 1440
        type: "calico"
        version: "v3.22.4"
      containerRuntime:
        insecureRegistry: []
        rootDir: "/var/lib/containerd"
        type: "containerd"
        version: "1.6.4"
      etcd:
        dataDir: "/var/lib/etcd"
      kind: "Cluster"
      kubeProxy: {}
      kubelet:
        rootDir: "/var/lib/kubelet"
      kubernetesVersion: "v1.23.6"
      localRegistry: ""
      masters:
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[0].metadata.name']
          label: {}
          taints: []
      metadata:
        annotations:
          kubeclipper.io/offline: ""
        labels:
          topology.kubeclipper.io/region: "default"
        name: *cluster_name
      networking:
        dnsDomain: "cluster.local"
        ipFamily: "ipvs"
        pods:
          cidrBlocks:
            - "10.96.0.0/16"
        proxyMode: "ipvs"
        services:
          cidrBlocks:
            - "172.25.0.0/24"
        workerNodeVip: "169.254.169.100"
      provider:
        name: "kubeadm"
      workers: []
    response_json_paths:
      $.masters.[0].id: $HISTORY['get_available_node'].$RESPONSE['$.items[0].metadata.name']

  - name: creat_namesake_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters
    method: POST
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data:
      addons: []
      apiVersion: "core.kubeclipper.io/v1"
      certSANs: []
      cni:
        calico:
          IPManger: true
          IPv4AutoDetection: "first-found"
          IPv6AutoDetection: ""
          mode: "Overlay-Vxlan-All"
          mtu: 1440
        type: "calico"
        version: "v3.22.4"
      containerRuntime:
        insecureRegistry: []
        rootDir: "/var/lib/containerd"
        type: "containerd"
        version: "1.6.4"
      etcd:
        dataDir: "/var/lib/etcd"
      kind: "Cluster"
      kubeProxy: {}
      kubelet:
        rootDir: "/var/lib/kubelet"
      kubernetesVersion: "v1.23.6"
      localRegistry: ""
      masters:
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[0].metadata.name']
          label: {}
          taints: []
      metadata:
        annotations:
          kubeclipper.io/offline: ""
        labels:
          topology.kubeclipper.io/region: "default"
        name: *cluster_name
      networking:
        dnsDomain: "cluster.local"
        ipFamily: "ipvs"
        pods:
          cidrBlocks:
            - "10.96.0.0/16"
        proxyMode: "ipvs"
        services:
          cidrBlocks:
            - "172.25.0.0/24"
        workerNodeVip: "169.254.169.100"
      provider:
        name: "kubeadm"
      workers: []
    status: 400
    response_strings:
      - "already exists"

  - name: wait_cluster_status_is_running
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    poll:
      count: 60
      delay: 10
    response_json_paths:
      $.status.phase: "Running"

  - name: add_one_node
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']/nodes
    method: PUT
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data:
      nodes:
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[1].metadata.name']
          taints: []
          labels: {}
      operation: "add"
      role: "worker"
    response_json_paths:
      $.status.phase: "Updating"

  - name: check_cluster_status_is_running
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    poll:
      count: 60
      delay: 10
    response_json_paths:
      $.status.phase: "Running"

  - name: check_add_node_sucessfully
    url: /apis/api/core.kubeclipper.io/v1/nodes
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    query_parameters:
      labelSelector: kubeclipper.io/cluster=$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
      limit: 10
      page: 1
      reverse: false
      silent: false
    status: 200
    response_json_paths:
      $.totalCount: 2

  - name: remove_one_worker_node
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']/nodes
    method: PUT
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data:
      nodes:
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[1].metadata.name']
      operation: "remove"
      role: "worker"
    response_json_paths:
      $.status.phase: "Updating"

  - name: check_cluster_status_is_running
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    poll:
      count: 60
      delay: 10
    response_json_paths:
      $.status.phase: "Running"

  - name: check_remove_node_sucessfully
    url: /apis/api/core.kubeclipper.io/v1/nodes
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    query_parameters:
      labelSelector: kubeclipper.io/cluster=$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
      limit: 10
    status: 200
    response_json_paths:
      $.totalCount: 1

  - name: add_two_nodes
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']/nodes
    method: PUT
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data:
      nodes:
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[1].metadata.name']
          taints: []
          labels: {}
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[2].metadata.name']
          taints: []
          labels: {}
      operation: "add"
      role: "worker"
    response_json_paths:
      $.status.phase: "Updating"

  - name: check_batch_cluster_status_is_running
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    poll:
      count: 60
      delay: 10
    response_json_paths:
      $.status.phase: "Running"

  - name: check_add_two_nodes_sucessfully
    url: /apis/api/core.kubeclipper.io/v1/nodes
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    query_parameters:
      labelSelector: kubeclipper.io/cluster=$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
      limit: 10
      page: 1
      reverse: false
      silent: false
    status: 200
    response_json_paths:
      $.totalCount: 3

  - name: remove_two_worker_nodes
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']/nodes
    method: PUT
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data:
      nodes:
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[1].metadata.name']
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[2].metadata.name']
      operation: "remove"
      role: "worker"
    response_json_paths:
      $.status.phase: "Updating"

  - name: check_batch_cluster_status_is_running
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    poll:
      count: 60
      delay: 10
    response_json_paths:
      $.status.phase: "Running"

  - name: check_remove_nodes_sucessfully
    url: /apis/api/core.kubeclipper.io/v1/nodes
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    query_parameters:
      labelSelector: kubeclipper.io/cluster=$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
      limit: 10
    status: 200
    response_json_paths:
      $.totalCount: 1

  - name: delete_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
    method: DELETE
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    status: 200
