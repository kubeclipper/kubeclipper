# 创建nfs集群时，集群名称是 "nfs_cluster_name"，要改名字需要全局搜索 "nfs_cluster_name"
fixtures:
  - ConfigFixture
  - SampleDataFixture

defaults:
  ssl: False
  request_headers:
    content-type: application/json
    accept: application/json

vars:
  - &username 'admin'
  - &password 'Thinkbig1'
  - &cluster_name 'apitest_cluster_name'
  - &high_availability_cluster_name 'high_availability_name'
  - &localRegistry '172.20.139.239:5000'
  - &nfs_serverAddr '172.20.151.105'
  - &nfs_sharedPath '/tmp/nfs/data/mcc-host'

tests:
  - name: user_login
    url: /apis/oauth/login
    method: POST
    data:
      username: *username
      password: *password
    status: 200
    response_json_paths:
      $.token_type: Bearer

  - name: get_available_node
    url: /apis/api/core.kubeclipper.io/v1/nodes
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    query_parameters:
      labelSelector: topology.kubeclipper.io/region=default,!kubeclipper.io/nodeRole
      limit: -1
      page: 1
    status: 200

  - name: create_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters
    method: POST
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data:
      addons: []
      apiVersion: "core.kubeclipper.io/v1"
      certSANs: []
      cni: 
        calico: 
          IPManger: true
          IPv4AutoDetection: "first-found"
          IPv6AutoDetection: ""
          mode: "Overlay-Vxlan-All"
          mtu: 1440
        type: "calico"
        version: "v3.22.4"
      containerRuntime:
        insecureRegistry: []
        rootDir: "/var/lib/containerd"
        type: "containerd"
        version: "1.6.4"
      etcd:
        dataDir: "/var/lib/etcd"
      kind: "Cluster"
      kubeProxy: {}
      kubelet: 
        rootDir: "/var/lib/kubelet"
      kubernetesVersion: "v1.23.6"
      localRegistry: ""
      masters: 
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[0].metadata.name']
          label: {}
          taints: []
      metadata:
        annotations:
          kubeclipper.io/offline: ""
        labels:
          topology.kubeclipper.io/region: "default"
        name: *cluster_name
      networking:
        dnsDomain: "cluster.local"
        ipFamily: "ipvs"
        pods: 
          cidrBlocks: 
            - "10.96.0.0/16"
        proxyMode: "ipvs"
        services: 
          cidrBlocks:
            - "172.25.0.0/24"
        workerNodeVip: "169.254.169.100"
      provider: 
        name: "kubeadm"
      workers: []
    response_json_paths:
      $.masters.[0].id: $HISTORY['get_available_node'].$RESPONSE['$.items[0].metadata.name']

  - name: creat_namesake_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters
    method: POST
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data:
      addons: []
      apiVersion: "core.kubeclipper.io/v1"
      certSANs: []
      cni: 
        calico: 
          IPManger: true
          IPv4AutoDetection: "first-found"
          IPv6AutoDetection: ""
          mode: "Overlay-Vxlan-All"
          mtu: 1440
        type: "calico"
        version: "v3.22.4"
      containerRuntime:
        insecureRegistry: []
        rootDir: "/var/lib/containerd"
        type: "containerd"
        version: "1.6.4"
      etcd:
        dataDir: "/var/lib/etcd"
      kind: "Cluster"
      kubeProxy: {}
      kubelet: 
        rootDir: "/var/lib/kubelet"
      kubernetesVersion: "v1.23.6"
      localRegistry: ""
      masters: 
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[0].metadata.name']
          label: {}
          taints: []
      metadata:
        annotations:
          kubeclipper.io/offline: ""
        labels:
          topology.kubeclipper.io/region: "default"
        name: *cluster_name
      networking:
        dnsDomain: "cluster.local"
        ipFamily: "ipvs"
        pods: 
          cidrBlocks: 
            - "10.96.0.0/16"
        proxyMode: "ipvs"
        services: 
          cidrBlocks:
            - "172.25.0.0/24"
        workerNodeVip: "169.254.169.100"
      provider: 
        name: "kubeadm"
      workers: []
    status: 400
    response_strings:
      - "already exists"

  - name: check_cluster_status
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    poll:
      count: 60
      delay: 10
    response_json_paths:
      $.status.phase: "Running"
      
  - name: delete_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_cluster'].$RESPONSE['$.metadata.name']
    method: DELETE
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    status: 200

  - name: create_nfs_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters
    method: POST
    poll:
      count: 6
      delay: 10
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data: 
      kind: Cluster
      apiVersion: core.kubeclipper.io/v1
      metadata:
        name: nfs_cluster_name
        labels:
          topology.kubeclipper.io/region: default
        annotations:
          kubeclipper.io/offline: ''
          kubeclipper.io/description: cluster_description
      provider:
        name: kubeadm
      certSANs: []
      masters:
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[0].metadata.name']
          taints: []
          labels: {}
      workers: []
      localRegistry: *localRegistry
      kubernetesVersion: v1.23.6
      containerRuntime:
        type: containerd
        version: 1.6.4
        insecureRegistry:
          - *localRegistry
        rootDir: /var/lib/containerd
      networking:
        ipFamily: IPv4
        services:
          cidrBlocks:
            - 172.25.0.0/24
        dnsDomain: cluster.local
        pods:
          cidrBlocks:
            - 10.96.0.0/16
        workerNodeVip: 169.254.169.100
        proxyMode: ipvs
      kubeProxy: {}
      etcd:
        dataDir: /var/lib/etcd
      kubelet:
        rootDir: /var/lib/kubelet
      cni:
        type: calico
        version: v3.22.4
        calico:
          IPv4AutoDetection: first-found
          IPv6AutoDetection: ''
          mode: Overlay-Vxlan-All
          IPManger: true
          mtu: 1440
      addons:
        - name: nfs-csi
          config:
            serverAddr: *nfs_serverAddr
            sharedPath: *nfs_sharedPath
            scName: nfs-sc
            reclaimPolicy: Delete
            replicas: 1
            isDefaultSC: false
          version: v1
    response_json_paths:
      $.localRegistry: *localRegistry
      $.addons[0].config.reclaimPolicy: "Delete"
      $.addons[0].config.serverAddr: *nfs_serverAddr
      $.addons[0].config.sharedPath: *nfs_sharedPath
  
  - name: check_nfs_cluster_status
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_nfs_cluster'].$RESPONSE['$.metadata.name']
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    poll:
      count: 60
      delay: 10
    response_json_paths:
      $.status.phase: "Running"
  
  - name: cluster_list
    url: /apis/api/core.kubeclipper.io/v1/clusters?fieldSelector=&limit=10
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    status: 200
    response_json_paths:
      $.items[?(@.metadata.name=='nfs_cluster_name')].status.phase: Running
      $.items[?(@.metadata.name=='nfs_cluster_name')].addons[0].config.serverAddr: *nfs_serverAddr
      $.items[?(@.metadata.name=='nfs_cluster_name')].metadata.labels['topology.kubeclipper.io/region']: default
      $.items[?(@.metadata.name=='nfs_cluster_name')].metadata.annotations['kubeclipper.io/description']: cluster_description

  - name: delete_nfs_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters/nfs_cluster_name
    method: DELETE
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    status: 200

  - name: create_high_availability_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters
    method: POST
    poll:
      count: 60
      delay: 5
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    data:
      addons: []
      apiVersion: "core.kubeclipper.io/v1"
      certSANs: []
      cni: 
        calico: 
          IPManger: true
          IPv4AutoDetection: "first-found"
          IPv6AutoDetection: ""
          mode: "Overlay-Vxlan-All"
          mtu: 1440
        type: "calico"
        version: "v3.22.4"
      containerRuntime:
        insecureRegistry: []
        rootDir: "/var/lib/containerd"
        type: "containerd"
        version: "1.6.4"
      etcd:
        dataDir: "/var/lib/etcd"
      kind: "Cluster"
      kubeProxy: {}
      kubelet: 
        rootDir: "/var/lib/kubelet"
      kubernetesVersion: "v1.23.6"
      localRegistry: ""
      masters: 
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[0].metadata.name']
          label: {}
          taints: []
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[1].metadata.name']
          label: {}
          taints: []
        - id: $HISTORY['get_available_node'].$RESPONSE['$.items[2].metadata.name']
          label: {}
          taints: []
      metadata:
        annotations:
          kubeclipper.io/offline: ""
        labels:
          topology.kubeclipper.io/region: "default"
        name: *high_availability_cluster_name
      networking:
        dnsDomain: "cluster.local"
        ipFamily: "ipvs"
        pods: 
          cidrBlocks: 
            - "10.96.0.0/16"
        proxyMode: "ipvs"
        services: 
          cidrBlocks:
            - "172.25.0.0/24"
        workerNodeVip: "169.254.169.100"
      provider: 
        name: "kubeadm"
      workers: []
    response_json_paths:
      $.masters.[0].id: $HISTORY['get_available_node'].$RESPONSE['$.items[0].metadata.name']

  - name: check_high_availability_cluster_status
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_high_availability_cluster'].$RESPONSE['$.metadata.name']
    method: GET
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    poll:
      count: 60
      delay: 10
    response_json_paths:
      $.status.phase: "Running"

  - name: delete_high_availability_cluster
    url: /apis/api/core.kubeclipper.io/v1/clusters/$HISTORY['create_high_availability_cluster'].$RESPONSE['$.metadata.name']
    method: DELETE
    request_headers:
      Authorization: Bearer $HISTORY['user_login'].$RESPONSE['$.access_token']
    status: 200